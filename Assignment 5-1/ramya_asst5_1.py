# -*- coding: utf-8 -*-
"""Ramya_ASST5-1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NEUUy4yzxrX22tfe9KmoqjCgw4NHfVEv
"""

!pip install transformers

pip install datasets

pip install tensorflow

import pandas as pd
from transformers import TFAutoModel , AutoTokenizer
from datasets import load_dataset
from transformers import DistilBertTokenizer
from sklearn.model_selection import train_test_split
import tensorflow as tf

model = TFAutoModel.from_pretrained("bert-base-uncased")

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

input = tokenizer(['HR Dashboard'], padding = True, truncation = True,
                  return_tensors ='tf')
input

output = model(input)
output

emotions = pd.read_csv('/content/layoffs.csv')

emotions

def tokenize(batch) :
  return tokenizer(batch["text"], padding=True, truncation=True)

# Load DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Define a function to tokenize a single text
def tokenize_text(text):
    return tokenizer.encode_plus(text,
                                  add_special_tokens=True,
                                  max_length=512,
                                  padding='max_length',
                                  truncation=True,
                                  return_attention_mask=True,
                                  return_tensors='pt')

# Apply the function to each row of your DataFrame
emotions_encoded = emotions['country'].apply(tokenize_text)

# Now you have a Series of tokenized representations

emotions_encoded

import tensorflow as tf

class BERTForClassification(tf.keras.Model):

    def __init__(self, bert_model, num_classes):
        super().__init__()
        self.bert = bert_model
        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.bert(inputs)[1]
        return self.fc(x)

# Features
X = emotions[['industry', 'country', 'date']]

# Labels
y = emotions['total_laid_off']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)

# Now you can use X_train and y_train for training

classifier = BERTForClassification(model, num_classes=6)

classifier.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy']
)

# Check unique values in the 'industry' column
print("Unique values in 'industry':", X_train['industry'].unique())

# Check unique values in the 'country' column
print("Unique values in 'country':", X_train['country'].unique())

# Check if the columns exist before applying transformations
if 'percentage_laid_off' in emotions.columns:
    emotions['total_laid_off'] = emotions['percentage_laid_off'].apply(lambda x: 1 if x == 'Yes' else 0)
else:
    print("Column 'percentage_laid_off' not found in DataFrame.")

emotions.head(4)

emotions.hist(bins = 6, figsize = (5,5), color = 'y')

import matplotlib.pyplot as plt
emotions.groupby('industry')['percentage_laid_off'].mean().sort_values(ascending=False).plot(kind='bar')
plt.xlabel('Industry')
_ = plt.ylabel('Average Percentage Laid Off')

emotions.groupby('industry')['location'].count().plot(kind='bar')

import tensorflow as tf

# Select only numeric features for X_train
numeric_features = X_train.select_dtypes(include=['number'])

# Convert DataFrame to TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((dict(numeric_features), y_train))

# Batch the dataset
batch_size = 32
train_dataset = train_dataset.batch(batch_size)

classifier.summary()

import tensorflow as tf

# Convert DataFrame to TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))

# Batch the dataset
batch_size = 32
train_dataset = train_dataset.batch(batch_size)

# Convert DataFrame to TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train)).batch(batch_size)

# Train the classifier
history = classifier.fit(
    train_dataset,
    epochs=3
)